---
title: "Lab 8: Machine Learning Tuning"
author: Leila Aburomia
format: html
execute: 
  echo: true
---

```{r}
library(tidyverse)
library(tidymodels)
library(dplyr)
library(visdat)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(purrr)
library(ggplot2)
library(ggpubr)

```

# Data Cleaning

```{r}
# Data Download 
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

# Documentation PDF
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

# a. Basin characteristics
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# b. Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# b. where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

# c. 
walk2(remote_files, local_files, download.file, quiet = TRUE)

# d. Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE)

library(powerjoin)
camels <- power_full_join(camels ,by = 'gauge_id')

print(camels)
colSums(is.na(camels))

camels <- camels %>%
  mutate(across(
    where(is.numeric) & !all_of("geol_2nd_class"),
    ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)
  ))

camels <- camels %>%
  mutate(geol_2nd_class = if_else(is.na(geol_2nd_class), "Unknown", geol_2nd_class))

vis_dat(camels)

colSums(sapply(camels, is.infinite))
```

# Data Spliting

```{r}
library(rsample)
set.seed(932003)
c_split <- initial_split(camels, prop = 0.8)
camels_training <- training(c_split)
camels_testing  <- testing(c_split)
cam_cv <- vfold_cv(camels_training, v = 10)
```

# Feature Engineering 

```{r}
library(recipes)

camels <- camels |> 
  mutate(logQmean = log(q_mean))

flow_recipe2 <- recipe(q_mean ~ high_prec_freq + q95, data = camels_training) %>%
  step_naomit() %>%
  step_mutate(across(where(is.character), as.factor)) %>%
  step_mutate(across(where(is.factor), as.numeric))

flow_recipe_prep2 <- prep(flow_recipe2, training = camels_training)

bake_data <- juice(flow_recipe_prep2)

```

## Build 3 Candidate Models
```{r}
library(baguette)

forest_model2 <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")

forest_wf2 <- workflow() %>%
  add_recipe(flow_recipe2) %>%
  add_model(forest_model2)


lm_model2 <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

lm_wf2 <- workflow() %>%
  add_recipe(flow_recipe2) %>%
  add_model(lm_model2)


library(parsnip)
library(xgboost)

xg_model2 <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xg_wf2 <- workflow() %>%
  add_recipe(flow_recipe2) %>%
  add_model(xg_model2) 

```

## Test the Models 
```{r}
wf4 <- workflow_set(
  preproc = list(flow_recipe2), 
  models = list(forest2 = forest_model2, lm2 = lm_model2, xgboost2 = xg_model2))

results2 <- workflow_map(wf4, "fit_resamples", resamples = cam_cv)

autoplot(results2)

rank_results(results2, select_best = TRUE)

forest_wf2 <- workflow() %>%
  add_recipe(flow_recipe2) %>%
  add_model(forest_model2) %>%
  fit(data = camels_training)

forest_data2 <- augment(forest_wf2, new_data = camels_testing)
dim(forest_data2)
#> [1] 168 60
metrics(forest_data2, truth = q_mean, estimate = .pred)
```

## Model Selection
The Random Forest Model is best because it has a low 'rmse' and a high R^squared mean of .93. Additionally, the standard errors are less than other models.
The Random Forest Model uses the regression mode and ranger engine. It is preforming well for this problem because it handles the non linearity of streamflow well by modeling the interactions without them needing to be defined. It is also able to capture interactions between the input variables in my recipe. Lastly, the model is resistant to overfitting when compared to single models. 

# Model Tuning 
## 1. Build a model for your chosen specifications
```{r}
library(tidymodels)

forest_model2 <- rand_forest(
  mtry = tune(),      
  min_n = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("regression")

```

## 2. Create a Workflow
```{r}
forest_wf2 <- workflow() %>%
  add_recipe(flow_recipe2) %>%
  add_model(forest_model2)
```

## 3. Check The Tunable Values / Ranges
```{r}
dials <- extract_parameter_set_dials(forest_wf2)

dials

dials$object
```

## 4. Define the Search Space
```{r}
install.packages("dials")
library(dials)
library(tidymodels)

dials_final <- finalize(dials, flow_recipe2 %>% prep() %>% juice())

my.grid <- grid_space_filling(
  dials_final,
  size = 25,
  method = "lhs"
)

my.grid
```

## 5. Tune the Model
```{r}
model_params <-  tune_grid(
    forest_wf2,
    resamples = cam_cv,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

autoplot(model_params)
```
The results of my autoplot visualization show how different combinations of my hyperparameters affect the random forest tree model performance. For the Y-axis performance metric, my mae error is lower with values closer to 0. My rmse performance metric is also low indicating a lower error. The rsq values are very close to 1 showing that the model's predictions match the actual data well. 

## 6. Check the skill of the tuned model
```{r}
metrics_tuned <- collect_metrics(model_params)

metrics_tuned
```
The tibble shows a metric column that has the evaluation metric used, like rmse, rsq, or mae. There is a mean column that shows the average value of the metric for each set of hyperparameters. Which is the avg rmse, r squared, and mae across all cross-validation folds. n includes my chosen number of resamples which was set to be 10. There is also a std_error column showing the standard error of the metric and indicating how much variability there is in the performance across resamples. 

```{r}

best_mae <- show_best(model_params, metric = "mae", n = 1)

best_mae
```
The best model for Mean Absolute Error (mae) has the hyperparameters mtry = 2, the model considers 2 predictors at each split and min_n = 5 where the minimum number of observations required to make a split is 16. The avg mean for this hyperparameter combination is .256 which means that the model predictions are off by an average of .256 units from the actual values. The standard error is 0.116 meaning that the mae is very stable across the 10 resamples. 
```{r}
hp_best <- select_best(model_params, metric = "mae")

hp_best
```
## 7. Finalize your model
```{r}
final_wf <- finalize_workflow(
  forest_wf2,
  hp_best
)
```

# Final Model Verification
```{r}
final_fit <- last_fit(
  final_wf,
  split = c_split  
)

collect_metrics(final_fit)
```
The root mean squared error is slightly smaller (0.413) than the training rmse which means it is producing more accurate predictions. The R-sqaured is 0.94 which explains 94% of the variability in the test data. This value is also very close to the training performance, suggesting that the model preforms well without over fitting. These are extremely strong results, especially in streamflow predictions where variance can be high. 

```{r}
preds <- collect_predictions(final_fit)

preds
```
```{r}
library(ggplot2)


ggplot(preds, aes(x = q_mean, y = .pred)) +
  geom_point(aes(color = .pred), alpha = 0.7, size = 3) +   
  geom_smooth(method = "lm", se = FALSE, color = "green4", linewidth = 1.2) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") + 
  scale_color_viridis_c(option = "mako") +                      
  labs(
    title = "Predicted vs. Actual Streamflow",
    x = "Actual Values (Truth)",
    y = "Predicted Values",
    color = "Predicted"
  ) +
  theme_minimal(base_size = 14)

```

# Building a Map 
```{r}

final_model <- fit(final_wf, data = camels)
predictions <- augment(final_model, new_data = camels)

predictions <- predictions %>%
  mutate(residuals = (q_mean - .pred)^2)

pred_map <- ggplot(predictions, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +
  borders("state", colour = "gray50") +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(option = "plasma") +
  labs(title = "Predicted Streamflow Values",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal()


residual_map <- ggplot(predictions, aes(x = gauge_lon, y = gauge_lat, color = residuals)) + 
  borders("state", colour = "gray50") +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(option = "magma") +
  labs(title = "Residuals (Squared Differences)",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal()

library(patchwork)

final_plot <- pred_map + residual_map + plot_layout(ncol = 2)

final_plot
```

