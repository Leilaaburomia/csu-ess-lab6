[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "lab 6",
    "section": "",
    "text": "# Lab Set Up\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'forcats' was built under R version 4.4.2\n\n\nWarning: package 'lubridate' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'infer' was built under R version 4.4.2\n\n\nWarning: package 'modeldata' was built under R version 4.4.2\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.2\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'workflowsets' was built under R version 4.4.2\n\n\nWarning: package 'yardstick' was built under R version 4.4.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\n# Data Download \nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n# Documentation PDF\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n# a. Basin characteristics\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# b. Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# b. where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n# c. \nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# d. Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# e. merge tables \nlibrary(powerjoin)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nZero_q_freq is the frequency of days where discharge equals 0 mm/day (Q = 0 mm/day). It is measured in percentage."
  },
  {
    "objectID": "lab6.html#question-1.-download-data",
    "href": "lab6.html#question-1.-download-data",
    "title": "lab 6",
    "section": "",
    "text": "# Lab Set Up\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'forcats' was built under R version 4.4.2\n\n\nWarning: package 'lubridate' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'infer' was built under R version 4.4.2\n\n\nWarning: package 'modeldata' was built under R version 4.4.2\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.2\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'workflowsets' was built under R version 4.4.2\n\n\nWarning: package 'yardstick' was built under R version 4.4.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\n# Data Download \nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n# Documentation PDF\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n# a. Basin characteristics\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# b. Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# b. where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n# c. \nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# d. Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# e. merge tables \nlibrary(powerjoin)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nZero_q_freq is the frequency of days where discharge equals 0 mm/day (Q = 0 mm/day). It is measured in percentage."
  },
  {
    "objectID": "lab6.html#question-2.-make-2-maps",
    "href": "lab6.html#question-2.-make-2-maps",
    "title": "lab 6",
    "section": "Question 2. Make 2 Maps",
    "text": "Question 2. Make 2 Maps\n\n# map colored by aridity \np_aridity &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray40\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"lightgreen\", high = \"maroon\") +\n  ggthemes::theme_map() +\n  theme(legend.title = element_text(size = 7)) +\n  labs(color = \"Aridity (PET/P)\")\n  \n\n# map colored by p_mean\np_p_mean &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray40\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"yellow\", high = \"blue3\") +\n  ggthemes::theme_map() +\n  theme(legend.title = element_text(size = 7)) +\n  labs(color = \"Mean Precipitation (mm/day)\")\n \n\n\ncombined_plot &lt;- ggarrange(p_aridity, p_p_mean, ncol = 2, nrow = 1)\n\n\nannotate_figure(combined_plot, top = text_grob(\"Aridity and Mean Daily Precipitation Across the U.S.\", face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\nModel Preparation\n\ncamels %&gt;%\n  select(aridity, p_mean, q_mean) %&gt;%\n  drop_na() %&gt;%\n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n\nVisual EDA\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n## log-log relationship between aridity and rainfall is more linear\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# visualize how a log transform may benefit the q_mean data\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nModel Building\n\nlibrary(tidymodels)\n# 1. Split Data\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nModel Evaluation: statistical and visual\n\n# Method is error prone and worthless if wanted to test a different algorithm. \nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n\nUsing a workflow instead\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# replicate the results from the Lm_base model\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# Making predictions on the test data\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\n\n2nd Model Evaluation: statistical and visual\n\n# statistical\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n# Visual\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nAdvantage of Approach: can easily switch\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n# Predictions on the test data\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n# evaluate data and visual the observed vs predicted values colored by aridity\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n# workflowset approach \nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2"
  },
  {
    "objectID": "lab6.html#question-3.-build-a-xgboost-and-neural-network-model",
    "href": "lab6.html#question-3.-build-a-xgboost-and-neural-network-model",
    "title": "lab 6",
    "section": "Question 3. Build a xgboost and neural network model",
    "text": "Question 3. Build a xgboost and neural network model\nXgboost regression model using Boost_tree:\n\nlibrary(workflows)\nlibrary(dplyr)\nlibrary(parsnip)\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# model definition \nbt_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n# workflow\nbt_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(bt_model) %&gt;%\n  fit(data = camels_train)\n# Making predictions \nbt_data &lt;- augment(bt_wf, new_data = camels_test)\ndim(bt_data)\n\n[1] 135  60\n\n# Model Evaluations\nmetrics(bt_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.631\n2 rsq     standard       0.702\n3 mae     standard       0.397\n\nggplot(bt_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nNeural Network Model using nnet engine:\n\nlibrary(nnet)\n\nWarning: package 'nnet' was built under R version 4.4.3\n\nlibrary(baguette)\n\n# model definition\nmlp_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\", times = 10) %&gt;%\n  set_mode(\"regression\")\n# workflow\nmlp_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(mlp_model) %&gt;%\n  fit(data = camels_train)\n\n# Making predictions\nmlp_data &lt;- augment(mlp_wf, new_data = camels_test)\ndim(mlp_data)\n\n[1] 135  61\n\n# Model Evaluation\nmetrics(mlp_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.553\n2 rsq     standard       0.766\n3 mae     standard       0.339\n\nggplot(mlp_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nComparing models using a workflow set:\n\nwf2 &lt;- workflow_set(list(rec), list(lm_model, rf_model, bt_model, mlp_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf2)\n\n\n\n\n\n\n\nrank_results(wf2, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.544  0.0289    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.789  0.0249    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.562  0.0251    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.773  0.0259    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\nFrom the results of autoplot and rank_results I would move forward with the neural network model."
  },
  {
    "objectID": "lab6.html#question-4.-lm-pipeline-to-predict-mean-streamflow",
    "href": "lab6.html#question-4.-lm-pipeline-to-predict-mean-streamflow",
    "title": "lab 6",
    "section": "Question 4. LM Pipeline to predict mean streamflow",
    "text": "Question 4. LM Pipeline to predict mean streamflow\n\n4.a. Data Prep/Data Splitting\n\ncamels %&gt;%\n  select(high_prec_freq, q95, q_mean) %&gt;%\n  mutate(across(everything(), ~ ifelse(is.infinite(.), NA, .))) %&gt;%\n  drop_na()\n\n# A tibble: 670 × 3\n   high_prec_freq   q95 q_mean\n            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1           13.0  6.37   1.70\n 2           20.6  7.12   2.17\n 3           17.2  6.85   1.82\n 4           18.9  8.01   2.03\n 5           20.1  8.10   2.18\n 6           13.5  8.67   2.41\n 7           17.5 10.1    2.73\n 8           19.2  8.44   2.28\n 9           20.4  6.87   1.82\n10           20.8  5.78   1.70\n# ℹ 660 more rows\n\nset.seed(932003)\n\ncamels_split2 &lt;- initial_split(camels, prop = 0.75)\ncamels_train2 &lt;- training(camels_split2)\ncamels_test2 &lt;- testing(camels_split2)\n\ncamels_cv2 &lt;- vfold_cv(camels_train2, v = 10)\n\n\ncamels %&gt;%\n  select(high_prec_freq, q95, q_mean) %&gt;%\n  drop_na() %&gt;%\n  cor()\n\n               high_prec_freq        q95     q_mean\nhigh_prec_freq      1.0000000 -0.6535573 -0.6687589\nq95                -0.6535573  1.0000000  0.9633320\nq_mean             -0.6687589  0.9633320  1.0000000\n\n\n\nggplot(camels, aes(x = high_prec_freq, y = q95)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n4.b. Recipe\n\nlibrary(tidymodels)\n\nflow_recipe &lt;- recipe(logQmean ~ high_prec_freq + q95, data = camels_train2) %&gt;%\n  step_naomit() %&gt;%\n  step_mutate(across(where(is.character), as.factor)) %&gt;%\n  step_mutate(across(where(is.factor), as.numeric))\n\nflow_recipe_prep &lt;- prep(flow_recipe, training = camels_train2)\n\nbaked_data2 &lt;- juice(flow_recipe_prep)\n\nsum(is.na(baked_data))\n\n[1] 0\n\nsum(is.infinite(as.matrix(baked_data)))\n\n[1] 0\n\n\nI have chosen this formula because both high_prec_freq and q95 are correlated with mean daily discharge but not as strongly correlated with each other. high precipitation frequency has a correlation coefficient of -0.67 and q95 has a correlation coefficient of 0.96. Additionally from the PDF, high_prec_freq is fairly likely to affect streamflow because the higher frequency of rainfall will increase the potential of this water to increase streamflow. Q95 is directly correlated with streamflow because it describes high flow and will contribute to the mean.\n\n\n4.c. Define 3 Models\n\nlibrary(baguette)\n\nforest_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nforest_wf &lt;- workflow() %&gt;%\n  add_recipe(flow_recipe) %&gt;%\n  add_model(forest_model)\n\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(flow_recipe) %&gt;%\n  add_model(lm_model)\n\n\nlibrary(parsnip)\nlibrary(xgboost)\n\nxg_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxg_wf &lt;- workflow() %&gt;%\n  add_recipe(flow_recipe) %&gt;%\n  add_model(xg_model) \n\n\n\n4.d. Workflow Set\n\nwf3 &lt;- workflow_set(\n  preproc = list(flow_recipe), \n  models = list(forest = forest_model, lm = lm_model, xgboost = xg_model))\n\nresults &lt;- workflow_map(wf3, \"fit_resamples\", resamples = camels_cv2)\n\n\n\n4.e. Evaluation\n\nautoplot(results)\n\n\n\n\n\n\n\nrank_results(results, select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id       .config    .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_forest  Preproces… rmse    0.291  0.0156    10 recipe       rand…     1\n2 recipe_forest  Preproces… rsq     0.934  0.0123    10 recipe       rand…     1\n3 recipe_xgboost Preproces… rmse    0.296  0.0241    10 recipe       boos…     2\n4 recipe_xgboost Preproces… rsq     0.935  0.0107    10 recipe       boos…     2\n5 recipe_lm      Preproces… rmse    0.760  0.0438    10 recipe       line…     3\n6 recipe_lm      Preproces… rsq     0.627  0.0181    10 recipe       line…     3\n\n\nThe Random Forest Model is best because it has a low ‘rmse’ and a high R^squared mean of .93. Additionally, the standard errors are less than other models.\n\n\n4.f. Extract and Evaluate\n\nforest_wf &lt;- workflow() %&gt;%\n  add_recipe(flow_recipe) %&gt;%\n  add_model(forest_model) %&gt;%\n  fit(data = camels_train2)\n\nforest_data &lt;- augment(forest_wf, new_data = camels_test2)\ndim(forest_data)\n\n[1] 168  60\n\n#&gt; [1] 168 60\nmetrics(forest_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.259\n2 rsq     standard       0.941\n3 mae     standard       0.199\n\nggplot(forest_data, aes(x = logQmean, y = .pred, colour = high_prec_freq)) +\n  scale_color_gradient(low = \"orange3\", high = \"darkblue\") +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(title = \"Random Forest Model: Observed vs Predicted Mean Streamflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"High Precipitation Frequency (days/yr)\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe results of my predicted versus observed log mean streamflow show that the random forest model with high prec frequency and q95 as predictors does a good job predicting log mean streamflow. From the ggplot you can see that the slope is ~1 indicating that predicted is strongly positively correlated with observed. Additionally, you can see that the points are closely clustered around the line, showing significance of the results. From the points, you can also see that high precipitation frequency is positively correlated with log mean streamflow."
  }
]